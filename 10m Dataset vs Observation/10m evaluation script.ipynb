{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1929c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pickle\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import pandas as pd\n",
    "from rasterio.transform import rowcol\n",
    "from zoneinfo import ZoneInfo  \n",
    "\n",
    "\n",
    "# Loading of the ICON 10m windspeed pickle file\n",
    "windspeed_10m_icon_table = pd.read_pickle(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\wind-speed-10m_icon_table.pickle\")\n",
    "\n",
    "# Data cleaning(The remove of duplicated and unik value contains columns)\n",
    "windspeed_10m_icon_table = windspeed_10m_icon_table.loc[:, ~windspeed_10m_icon_table.columns.duplicated()]\n",
    "windspeed_10m_icon_table = windspeed_10m_icon_table.loc[:, windspeed_10m_icon_table.apply(lambda col: col.nunique() > 1, axis=0)]\n",
    "\n",
    "# 🔹 Load of the corresponding excel files \n",
    "df_mapping_ICON_Station_ID = pd.read_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Station_ID_vs_ICON_code.csv\", sep=';')\n",
    "# 🔹 Verification of columns availability\n",
    "if not {\"Station_ID\", \"ICON_code\"}.issubset(df_mapping_ICON_Station_ID.columns):\n",
    "    raise ValueError(\"Les colonnes 'Station_ID' et 'ICON_code' sont absentes du fichier.\")\n",
    "# 🔹 Create a dict of corresponding{ICON_code: Station_ID}\n",
    "icon_to_station = dict(zip(df_mapping_ICON_Station_ID[\"ICON_code\"].astype(str), df_mapping_ICON_Station_ID[\"Station_ID\"]))\n",
    "# 🔹 Subtitute the name of the columns instead of index\n",
    "windspeed_10m_icon_table.rename(columns=icon_to_station, inplace=True)\n",
    "windspeed_10m_icon_table.index = windspeed_10m_icon_table.index.tz_localize(\"UTC\")\n",
    "\n",
    "#Loading of the observed 10m windspeed pickle file\n",
    "obs_10m_table = pd.read_pickle(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\wind-speed-10m_obs_table.pickle\")\n",
    "print(\"Before shift\")\n",
    "print(obs_10m_table[\"858148\"])\n",
    "\n",
    "#Set the time zone to UTC\n",
    "station_SASSCAL_list = [\n",
    "    \"39711\", \"46943\", \"47066\", \"361100\", \"858246\", \"21\", \"22\", \"23\", \"24\", \"25\",\n",
    "    \"67581\", \"67583\", \"67585\", \"67591\", \"67593\", \"68024\", \"68026\", \"68030\", \"68038\",\n",
    "    \"68148\", \"68151\", \"68226\", \"68320\", \"68325\", \"68328\", \"101\", \"102\", \"103\", \"104\",\n",
    "    \"105\", \"106\", \"108\", \"109\", \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\",\n",
    "    \"4230\", \"4756\", \"4806\", \"8893\", \"31195\", \"31196\", \"31197\", \"31198\", \"31199\",\n",
    "    \"31200\", \"31202\", \"31203\", \"31204\", \"31205\", \"31206\", \"31207\", \"31209\", \"31210\",\n",
    "    \"31212\", \"31213\", \"31214\", \"31215\", \"31216\", \"52121\", \"58557\", \"64243\", \"64258\",\n",
    "    \"65934\", \"65941\", \"67134\", \"67135\", \"74229\", \"74714\", \"75823\", \"80493\", \"80498\",\n",
    "    \"E7624\", \"E7625\", \"E7626\", \"E7627\", \"E7628\", \"E7629\", \"E7630\", \"E7631\",\n",
    "    \"E9126\", \"E9127\", \"361096\", \"361097\", \"361098\", \"361099\",\n",
    "    \"511942\", \"511943\", \"511944\",\n",
    "    \"858576\", \"858577\", \"858580\", \"858581\", \"858583\", \"858585\", \"858590\", \"858596\",\n",
    "    \"856134\", \"858148\", \"858594\"\n",
    "]\n",
    "E_cols = [\"E7624\", \"E7625\", \"E7626\", \"E7627\", \"E7628\", \"E7629\", \"E7630\", \"E7631\", \"E9126\", \"E9127\"]\n",
    "utc_plus_1_cols = [\"46943\", \"47066\", \"361100\", \"858246\"]\n",
    "remains_sasscal_cols = [col for col in station_SASSCAL_list if col not in utc_plus_1_cols and col not in E_cols]\n",
    "utc_plus_2_cols = [col for col in remains_sasscal_cols if col in obs_10m_table.columns]\n",
    "remains_obs_all = [col for col in obs_10m_table.columns if col not in utc_plus_1_cols and col not in utc_plus_2_cols]\n",
    "\n",
    "windspeed_10m_obs_table = pd.DataFrame()\n",
    "\n",
    "# For station already in UTC\n",
    "for col in remains_obs_all:\n",
    "    if col in obs_10m_table.columns:\n",
    "        s = obs_10m_table[col].copy()\n",
    "        s.index = s.index.tz_localize(\"UTC\")\n",
    "        windspeed_10m_obs_table[col] = s\n",
    "\n",
    "# For station in UTC+1 (like l'Angola)\n",
    "for col in utc_plus_1_cols:\n",
    "    if col in obs_10m_table.columns:\n",
    "        s = obs_10m_table[col].copy()\n",
    "        s.index = s.index.tz_localize(\"Africa/Luanda\").tz_convert(\"UTC\")\n",
    "        windspeed_10m_obs_table[col] = s\n",
    "\n",
    "# For station in UTC+2 (like l'Afrique du Sud)\n",
    "for col in utc_plus_2_cols:\n",
    "    if col in obs_10m_table.columns:\n",
    "        s = obs_10m_table[col].copy()\n",
    "        s.index = s.index.tz_localize(\"Africa/Johannesburg\").tz_convert(\"UTC\")\n",
    "        windspeed_10m_obs_table[col] = s      \n",
    "print(\"After shift\")\n",
    "print(windspeed_10m_obs_table[\"858148\"])\n",
    "\n",
    "#End of time zone setting\n",
    "\n",
    "#The remove of the implausible data\n",
    "windspeed_10m_obs_table = windspeed_10m_obs_table.where((windspeed_10m_obs_table >= 0) & (windspeed_10m_obs_table <= 40), other=pd.NA)\n",
    "# 🔹Definition of NaN % limit to delete column\n",
    "threshold = 0.70\n",
    "# 🔹 Supprimer les colonnes où PLUS DE `threshold` % des valeurs sont NaN\n",
    "windspeed_10m_obs_table = windspeed_10m_obs_table.dropna(axis=1, thresh=int((1 - threshold) * len(windspeed_10m_obs_table)))\n",
    "\n",
    "\n",
    "# ERA5 Start here\n",
    "stations = pd.read_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Stationlat&lon_new.csv\" , sep=';')\n",
    "\n",
    "ERA5_Wind_2017_10m = nc.Dataset(r\"D:\\Wascal_2023-2025\\Thesis\\2025-04-14\\wind_speed\\43df5441fac8134c5e81bd3aa0c48fb0\\data_stream-oper_stepType-instant.nc\")\n",
    "ERA5_Wind_2018_10m = nc.Dataset(r\"D:\\Wascal_2023-2025\\Thesis\\2025-04-14\\wind_speed\\f4cdecdedec5898827f229a7fb114bd5\\data_stream-oper_stepType-instant.nc\")\n",
    "ERA5_Wind_2019_10m = nc.Dataset(r\"D:\\Wascal_2023-2025\\Thesis\\2025-04-14\\wind_speed\\c832a1ecc417cc0e2e0a31ebed19cef6\\data_stream-oper_stepType-instant.nc\")\n",
    "wind_2017_u10 = ERA5_Wind_2017_10m.variables['u10'][:]\n",
    "wind_2017_v10 = ERA5_Wind_2017_10m.variables['v10'][:]\n",
    "wind_2018_u10 = ERA5_Wind_2018_10m.variables['u10'][:]\n",
    "wind_2018_v10 = ERA5_Wind_2018_10m.variables['v10'][:]\n",
    "wind_2019_u10 = ERA5_Wind_2019_10m.variables['u10'][:]\n",
    "wind_2019_v10 = ERA5_Wind_2019_10m.variables['v10'][:]\n",
    "\n",
    "wind_ERA5_list = []\n",
    "lats_2017 = ERA5_Wind_2017_10m['latitude'][:]\n",
    "lons_2017 = ERA5_Wind_2017_10m['longitude'][:]\n",
    "lats_2018 = ERA5_Wind_2018_10m['latitude'][:]\n",
    "lons_2018 = ERA5_Wind_2018_10m['longitude'][:]\n",
    "lats_2019 = ERA5_Wind_2019_10m['latitude'][:]\n",
    "lons_2019 = ERA5_Wind_2019_10m['longitude'][:]\n",
    "for _, row in stations.iterrows():\n",
    "    lat_idx_2017 = np.abs(lats_2017 - row['latitude']).argmin()\n",
    "    lon_idx_2017 = np.abs(lons_2017 - row['longitude']).argmin()\n",
    "    lat_idx_2018 = np.abs(lats_2018 - row['latitude']).argmin()\n",
    "    lon_idx_2018 = np.abs(lons_2018 - row['longitude']).argmin()\n",
    "    lat_idx_2019 = np.abs(lats_2019 - row['latitude']).argmin()\n",
    "    lon_idx_2019 = np.abs(lons_2019 - row['longitude']).argmin()\n",
    "    wind_2017_10m = np.sqrt(((wind_2017_u10[:, lat_idx_2017, lon_idx_2017])**2) + ((wind_2017_v10[:, lat_idx_2017, lon_idx_2017])**2))\n",
    "    wind_2018_10m = np.sqrt(((wind_2018_u10[:, lat_idx_2018, lon_idx_2018])**2) + ((wind_2018_v10[:, lat_idx_2018, lon_idx_2018])**2))\n",
    "    wind_2019_10m = np.sqrt(((wind_2019_u10[:, lat_idx_2019, lon_idx_2019])**2) + ((wind_2019_v10[:, lat_idx_2019, lon_idx_2019])**2))\n",
    "    wind_2017_to_2019_ERA5_10m = np.concatenate([wind_2017_10m, wind_2018_10m, wind_2019_10m], axis=0)\n",
    "    wind_ERA5_list.append(pd.Series(wind_2017_to_2019_ERA5_10m, name=row['station']))\n",
    "    \n",
    "wind_ERA5 = pd.concat(wind_ERA5_list, axis=1)\n",
    "start_time = \"2017-01-01 00:00:00\"\n",
    "wind_ERA5.index = pd.date_range(start=start_time, periods=wind_ERA5.shape[0], freq=\"h\").tz_localize(\"UTC\")\n",
    "print(wind_ERA5['858148'])\n",
    "\n",
    "# Extraction of ERA5 WM\n",
    "WM_Station = ['WM01', 'WM02', 'WM03', 'WM05', 'WM06', 'WM07', 'WM08', 'WM09', 'WM10', 'WM11', 'WM12', 'WM13', 'WM14', 'WM15', 'WM16', 'WM17', 'WM18', 'WM19']\n",
    "windspeed_10m_ERA5_WM = wind_ERA5[WM_Station]\n",
    "\n",
    "#ERA5_GWA start here\n",
    "ERA5_wind_speed_10m_mean_2008to2017_southern_Africa = nc.Dataset(r\"D:\\Wascal_2023-2025\\Thesis\\2025-04-14\\wind_speed\\ERA5_wind_speed_10m_mean_2008to2017_southern_Africa.nc\")\n",
    "lats_ERA5 = ERA5_wind_speed_10m_mean_2008to2017_southern_Africa.variables['latitude'][:]\n",
    "lons_ERA5 = ERA5_wind_speed_10m_mean_2008to2017_southern_Africa.variables['longitude'][:]\n",
    "wind_mean_ERA5 = ERA5_wind_speed_10m_mean_2008to2017_southern_Africa.variables['si10'][0, :, :]  # (time, lat, lon)\n",
    "\n",
    "# === Étape 1 : Load of CSV files of stations carateristics\n",
    "stations = pd.read_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Stationlat&lon_new.csv\" , sep=';')\n",
    "# === Étape 3 : Nearest grid finding\n",
    "def get_wind_value(lon, lat):\n",
    "    lat_idx = np.abs(lats_ERA5 - lat).argmin()\n",
    "    lon_idx = np.abs(lons_ERA5 - lon).argmin()\n",
    "    return float(wind_mean_ERA5[lat_idx, lon_idx])  # Convertir en float simple\n",
    "\n",
    "# === Étape 4 : Application on every stations\n",
    "stations['wind_mean_10m_ERA5'] = stations.apply(lambda row: get_wind_value(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "# === Load of raster GWA\n",
    "rds = rxr.open_rasterio(r\"C:\\Users\\LENOVO\\Downloads\\wind_speed_cog_10m.tif\",masked=True).squeeze()\n",
    "\n",
    "# Use of xarray for nesrest\n",
    "rds = rds.sortby(\"y\")\n",
    "\n",
    "# Verification\n",
    "print(\"📐 Dimensions raster :\", rds.dims)  # ('y', 'x') — bon !\n",
    "print(\"🧭 Coordonnées disponibles :\", list(rds.coords))  # ['x', 'y']\n",
    "\n",
    "# === Nearest methods\n",
    "interpolated = rds.interp(\n",
    "    x=(\"points\", stations[\"longitude\"].values),\n",
    "    y=(\"points\", stations[\"latitude\"].values),\n",
    "    method=\"nearest\"\n",
    ")\n",
    "\n",
    "# === Ajouter la colonne au tableau\n",
    "stations[\"wind_mean_10m_GWA_xarray_nearest\"] = interpolated.values\n",
    "\n",
    "stations.to_excel(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Stations_ERA5_GWA_mean_xarray_nearest.xlsx\", index=False)\n",
    "stations['scaling_factor'] = stations['wind_mean_10m_GWA_xarray_nearest'] / stations['wind_mean_10m_ERA5']\n",
    "stations.to_excel(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Station_scaling_ERA5_GWA_array_nearest.xlsx\", index=False)\n",
    "print(\"✅ Nearest methode avec xarray terminée !\")        \n",
    "\n",
    "wind_ERA5_GWA_list = []\n",
    "\n",
    "for _, row in stations.iterrows():\n",
    "    station_code = row['station']\n",
    "    if station_code in wind_ERA5.columns:\n",
    "        wind_ts = wind_ERA5[station_code]\n",
    "        wind_correction = wind_ts * row['scaling_factor']\n",
    "        wind_ERA5_GWA_list.append(pd.Series(wind_correction, name=station_code))\n",
    "\n",
    "# Fusionner les séries en un DataFrame\n",
    "wind_ERA5_GWA = pd.concat(wind_ERA5_GWA_list, axis=1)\n",
    "\n",
    "# Ajouter une index temporel\n",
    "start_time = \"2017-01-01 00:00:00\"\n",
    "wind_ERA5_GWA.index = pd.date_range(start=start_time, periods=wind_ERA5_GWA.shape[0], freq=\"h\").tz_localize(\"UTC\")\n",
    "print(wind_ERA5_GWA['858148'])\n",
    "#extraire les WM de ERA5\n",
    "windspeed_10m_ERA5_GWA_WM = wind_ERA5_GWA[WM_Station]\n",
    "\n",
    "# 🔹 Find identic column\n",
    "common_stations = list(set(windspeed_10m_obs_table.columns) & set(windspeed_10m_icon_table.columns) & set(wind_ERA5.columns) & set(wind_ERA5_GWA.columns))\n",
    "# 🔹 Filtrer les DataFrames pour ne garder que les stations communes\n",
    "windspeed_10m_obs_table = windspeed_10m_obs_table[common_stations]\n",
    "windspeed_10m_icon_table = windspeed_10m_icon_table[common_stations]\n",
    "windspeed_10m_ERA5_table = wind_ERA5[common_stations]\n",
    "windspeed_10m_ERA5_GWA_table = wind_ERA5_GWA[common_stations]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ICON-LAM and Observed data Dict creation\n",
    "obs_dict = {}   # Contiendra les données d'observation (WS_10_obs)\n",
    "icon_dict = {}  # Contiendra les données simulées ICON (WS_10_ICON)\n",
    "\n",
    "#  Conversion of dataframe into dict\n",
    "for station in windspeed_10m_obs_table.columns:\n",
    "    obs_dict[station] = pd.DataFrame({\"WS_10_obs\": windspeed_10m_obs_table[station]})\n",
    "\n",
    "for station in windspeed_10m_icon_table.columns:\n",
    "    icon_dict[station] = pd.DataFrame({\"WS_10_ICON\": windspeed_10m_icon_table[station]})\n",
    "\n",
    "#  Station information loading\n",
    "station_info = pd.read_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Station_ID_vs_ICON_code & lat_lon_Label.csv\", sep=';')\n",
    "\n",
    "required_columns = {\"Station_ID\", \"longitude\", \"latitude\", \"label\"}\n",
    "if not required_columns.issubset(station_info.columns):\n",
    "    raise ValueError(\"Les colonnes 'Station_ID', 'longitude', 'latitude', 'label' sont absentes du fichier CSV.\")\n",
    "\n",
    "station_meta = station_info.set_index(\"Station_ID\")[[\"longitude\", \"latitude\", \"label\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# dict configuration\n",
    "for station in obs_dict.keys():\n",
    "    if station in station_meta:\n",
    "        obs_dict[station][\"Station_ID\"] = station\n",
    "        obs_dict[station][\"longitude\"] = station_meta[station][\"longitude\"]\n",
    "        obs_dict[station][\"latitude\"] = station_meta[station][\"latitude\"]\n",
    "        obs_dict[station][\"label\"] = station_meta[station][\"label\"]\n",
    "\n",
    "for station in icon_dict.keys():\n",
    "    if station in station_meta:\n",
    "        icon_dict[station][\"Station_ID\"] = station\n",
    "        icon_dict[station][\"longitude\"] = station_meta[station][\"longitude\"]\n",
    "        icon_dict[station][\"latitude\"] = station_meta[station][\"latitude\"]\n",
    "        icon_dict[station][\"label\"] = station_meta[station][\"label\"]\n",
    "\n",
    "\n",
    "# ERA5 and ERA5_GWA dict creation\n",
    "ERA5_dict = {}   # Contiendra les données d'observation (WS_10_ERA5)\n",
    "ERA5_GWA_dict = {}   # Contiendra les données d'observation (WS_10_ERA5_GWA)\n",
    "# 🔹 Convertir les DataFrames en dictionnaires basés sur les colonnes actuelles (stations)\n",
    "for station in windspeed_10m_ERA5_table.columns:\n",
    "    ERA5_dict[station] = pd.DataFrame({\"WS_10_ERA5\": windspeed_10m_ERA5_table[station]})\n",
    "\n",
    "for station in windspeed_10m_ERA5_GWA_table.columns:\n",
    "    ERA5_GWA_dict[station] = pd.DataFrame({\"WS_10_ERA5_GWA\": windspeed_10m_ERA5_GWA_table[station]})\n",
    "\n",
    "# 🔹 Ajouter les métadonnées à chaque station dans les dictionnaires\n",
    "for station in ERA5_dict.keys():\n",
    "    if station in station_meta:\n",
    "        ERA5_dict[station][\"Station_ID\"] = station\n",
    "        ERA5_dict[station][\"longitude\"] = station_meta[station][\"longitude\"]\n",
    "        ERA5_dict[station][\"latitude\"] = station_meta[station][\"latitude\"]\n",
    "        ERA5_dict[station][\"label\"] = station_meta[station][\"label\"]\n",
    "        \n",
    "for station in ERA5_GWA_dict.keys():\n",
    "    if station in station_meta:\n",
    "        ERA5_GWA_dict[station][\"Station_ID\"] = station\n",
    "        ERA5_GWA_dict[station][\"longitude\"] = station_meta[station][\"longitude\"]\n",
    "        ERA5_GWA_dict[station][\"latitude\"] = station_meta[station][\"latitude\"]\n",
    "        ERA5_GWA_dict[station][\"label\"] = station_meta[station][\"label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047f6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Initialiser un dictionnaire pour stocker les résultats ICON\n",
    "metrics_ICON_dict = {}\n",
    "\n",
    "# 🔹 Calculer les métriques pour chaque station\n",
    "for station in obs_dict.keys():\n",
    "    if station in icon_dict:  # Vérifier que la station est présente dans les deux dictionnaires\n",
    "        obs_data = obs_dict[station][\"WS_10_obs\"].dropna()  # Supprimer les NaN\n",
    "        icon_data = icon_dict[station][\"WS_10_ICON\"].dropna()  # Supprimer les NaN\n",
    "\n",
    "        # 🔹 Aligner les données en fonction de l'index (temps)\n",
    "        obs_data.name = \"WS_10_obs\"\n",
    "        icon_data.name = \"WS_10_ICON\"\n",
    "        merged_df_ICON = pd.concat([obs_data, icon_data], axis=1, join=\"inner\")\n",
    "\n",
    "        if not merged_df_ICON.empty:\n",
    "            mae = np.mean(np.abs(merged_df_ICON[\"WS_10_ICON\"] - merged_df_ICON[\"WS_10_obs\"]))  # Mean Absolute Error\n",
    "            me = np.mean(merged_df_ICON[\"WS_10_ICON\"] - merged_df_ICON[\"WS_10_obs\"])  # Mean Error\n",
    "                        \n",
    "            # 🔹 Pearson r (calculé manuellement selon la formule souhaitée)\n",
    "            O = merged_df_ICON[\"WS_10_obs\"].values\n",
    "            M = merged_df_ICON[\"WS_10_ICON\"].values  # 2ᵉ colonne du modèle (ICON, ERA5 ou ERA5_GWA)\n",
    "\n",
    "            O_bar = np.mean(O)\n",
    "            M_bar = np.mean(M)\n",
    "            numerateur = np.sum((M - M_bar) * (O - O_bar))\n",
    "            denominateur = np.sqrt(np.sum((M - M_bar)**2)) * np.sqrt(np.sum((O - O_bar)**2))\n",
    "            pearson_r = numerateur / denominateur if denominateur != 0 else np.nan\n",
    "            \n",
    "            \n",
    "            hist_obs, bin_edges = np.histogram(merged_df_ICON[\"WS_10_obs\"], bins=20, density=True)\n",
    "            hist_icon, _ = np.histogram(merged_df_ICON[\"WS_10_ICON\"], bins=bin_edges, density=True)\n",
    "            pss = np.sum(np.minimum(hist_obs, hist_icon)) * np.diff(bin_edges).mean()\n",
    "\n",
    "            # 🔹 Stocker les résultats\n",
    "            metrics_ICON_dict[station] = {\n",
    "                \"Station_ID\": station,\n",
    "                \"longitude\": obs_dict[station][\"longitude\"].iloc[0],\n",
    "                \"latitude\": obs_dict[station][\"latitude\"].iloc[0],\n",
    "                \"label\": obs_dict[station][\"label\"].iloc[0],\n",
    "                \"MAE\": mae,\n",
    "                \"ME\": me,\n",
    "                \"Pearson r\": pearson_r,\n",
    "                \"PSS\": pss\n",
    "            }\n",
    "\n",
    "# 🔹 Convertir en DataFrame pour affichage et sauvegarde\n",
    "metrics_ICON_df = pd.DataFrame.from_dict(metrics_ICON_dict, orient=\"index\")\n",
    "# 🔹 Sauvegarder le DataFrame en CSV pour utilisation future (optionnel)\n",
    "metrics_ICON_df.to_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ICON_LAM_results.csv\", index=False)\n",
    "\n",
    "\n",
    "# 🔹 Initialiser un dictionnaire pour stocker les résultats ERA5\n",
    "metrics_ERA5_dict = {}\n",
    "\n",
    "# 🔹 Calculer les métriques pour chaque station\n",
    "for station in obs_dict.keys():\n",
    "    if station in ERA5_dict:  # Vérifier que la station est présente dans les deux dictionnaires\n",
    "        obs_data = obs_dict[station][\"WS_10_obs\"].dropna()  # Supprimer les NaN\n",
    "        ERA5_data = ERA5_dict[station][\"WS_10_ERA5\"].dropna()  # Supprimer les NaN\n",
    "\n",
    "        # 🔹 Aligner les données en fonction de l'index (temps)\n",
    "        obs_data.name = \"WS_10_obs\"\n",
    "        ERA5_data.name = \"WS_10_ERA5\"\n",
    "        merged_df_ERA5 = pd.concat([obs_data, ERA5_data], axis=1, join=\"inner\")\n",
    "\n",
    "        if not merged_df_ERA5.empty:\n",
    "            mae = np.mean(np.abs(merged_df_ERA5[\"WS_10_ERA5\"] - merged_df_ERA5[\"WS_10_obs\"]))  # Mean Absolute Error\n",
    "            me = np.mean(merged_df_ERA5[\"WS_10_ERA5\"] - merged_df_ERA5[\"WS_10_obs\"])  # Mean Error\n",
    "            \n",
    "            # 🔹 Pearson r (calculé manuellement selon la formule souhaitée)\n",
    "            O = merged_df_ERA5[\"WS_10_obs\"].values\n",
    "            M = merged_df_ERA5[\"WS_10_ERA5\"].values  # 2ᵉ colonne du modèle (ICON, ERA5 ou ERA5_GWA)\n",
    "\n",
    "            O_bar = np.mean(O)\n",
    "            M_bar = np.mean(M)\n",
    "            numerateur = np.sum((M - M_bar) * (O - O_bar))\n",
    "            denominateur = np.sqrt(np.sum((M - M_bar)**2)) * np.sqrt(np.sum((O - O_bar)**2))\n",
    "            pearson_r = numerateur / denominateur if denominateur != 0 else np.nan\n",
    "\n",
    "            \n",
    "            hist_obs, bin_edges = np.histogram(merged_df_ERA5[\"WS_10_obs\"], bins=20, density=True)\n",
    "            hist_icon, _ = np.histogram(merged_df_ERA5[\"WS_10_ERA5\"], bins=bin_edges, density=True)\n",
    "            pss = np.sum(np.minimum(hist_obs, hist_icon)) * np.diff(bin_edges).mean()\n",
    "\n",
    "            # 🔹 Stocker les résultats\n",
    "            metrics_ERA5_dict[station] = {\n",
    "                \"Station_ID\": station,\n",
    "                \"longitude\": obs_dict[station][\"longitude\"].iloc[0],\n",
    "                \"latitude\": obs_dict[station][\"latitude\"].iloc[0],\n",
    "                \"label\": obs_dict[station][\"label\"].iloc[0],\n",
    "                \"MAE\": mae,\n",
    "                \"ME\": me,\n",
    "                \"Pearson r\": pearson_r,\n",
    "                \"PSS\": pss\n",
    "            }\n",
    "\n",
    "# 🔹 Convertir en DataFrame pour affichage et sauvegarde\n",
    "metrics_ERA5_df = pd.DataFrame.from_dict(metrics_ERA5_dict, orient=\"index\")\n",
    "# 🔹 Sauvegarder le DataFrame en CSV pour utilisation future (optionnel)\n",
    "metrics_ERA5_df.to_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_results.csv\", index=False)\n",
    "\n",
    "\n",
    "# 🔹 Initialiser un dictionnaire pour stocker les résultats ERA5_GWA\n",
    "metrics_ERA5_GWA_dict = {}\n",
    "\n",
    "# 🔹 Calculer les métriques pour chaque station\n",
    "for station in obs_dict.keys():\n",
    "    if station in ERA5_GWA_dict:  # Vérifier que la station est présente dans les deux dictionnaires\n",
    "        obs_data = obs_dict[station][\"WS_10_obs\"].dropna()  # Supprimer les NaN\n",
    "        ERA5_GWA_data = ERA5_GWA_dict[station][\"WS_10_ERA5_GWA\"].dropna()  # Supprimer les NaN\n",
    "\n",
    "        # 🔹 Aligner les données en fonction de l'index (temps)\n",
    "        obs_data.name = \"WS_10_obs\"\n",
    "        ERA5_GWA_data.name = \"WS_10_ERA5_GWA\"\n",
    "        merged_df_ERA5_GWA = pd.concat([obs_data, ERA5_GWA_data], axis=1, join=\"inner\")\n",
    "\n",
    "        if not merged_df_ERA5.empty:\n",
    "            mae = np.mean(np.abs(merged_df_ERA5_GWA[\"WS_10_ERA5_GWA\"] - merged_df_ERA5_GWA[\"WS_10_obs\"]))  # Mean Absolute Error\n",
    "            me = np.mean(merged_df_ERA5_GWA[\"WS_10_ERA5_GWA\"] - merged_df_ERA5_GWA[\"WS_10_obs\"])  # Mean Error\n",
    "            \n",
    "            # 🔹 Pearson r (calculé manuellement selon la formule souhaitée)\n",
    "            O = merged_df_ERA5_GWA[\"WS_10_obs\"].values\n",
    "            M = merged_df_ERA5_GWA[\"WS_10_ERA5_GWA\"].values  # 2ᵉ colonne du modèle (ICON, ERA5 ou ERA5_GWA)\n",
    "\n",
    "            O_bar = np.mean(O)\n",
    "            M_bar = np.mean(M)\n",
    "            numerateur = np.sum((M - M_bar) * (O - O_bar))\n",
    "            denominateur = np.sqrt(np.sum((M - M_bar)**2)) * np.sqrt(np.sum((O - O_bar)**2))\n",
    "            pearson_r = numerateur / denominateur if denominateur != 0 else np.nan\n",
    "\n",
    "                        \n",
    "            hist_obs, bin_edges = np.histogram(merged_df_ERA5_GWA[\"WS_10_obs\"], bins=20, density=True)\n",
    "            hist_ERA5_GWA, _ = np.histogram(merged_df_ERA5_GWA[\"WS_10_ERA5_GWA\"], bins=bin_edges, density=True)\n",
    "            pss = np.sum(np.minimum(hist_obs, hist_ERA5_GWA)) * np.diff(bin_edges).mean()\n",
    "\n",
    "            # 🔹 Stocker les résultats\n",
    "            metrics_ERA5_GWA_dict[station] = {\n",
    "                \"Station_ID\": station,\n",
    "                \"longitude\": obs_dict[station][\"longitude\"].iloc[0],\n",
    "                \"latitude\": obs_dict[station][\"latitude\"].iloc[0],\n",
    "                \"label\": obs_dict[station][\"label\"].iloc[0],\n",
    "                \"MAE\": mae,\n",
    "                \"ME\": me,\n",
    "                \"Pearson r\": pearson_r,\n",
    "                \"PSS\": pss\n",
    "            }\n",
    "\n",
    "# 🔹 Convertir en DataFrame pour affichage et sauvegarde\n",
    "metrics_ERA5_GWA_df = pd.DataFrame.from_dict(metrics_ERA5_GWA_dict, orient=\"index\")\n",
    "# 🔹 Sauvegarder le DataFrame en CSV pour utilisation future (optionnel)\n",
    "metrics_ERA5_GWA_df.to_csv(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_GWA_results.csv\", index=False)\n",
    "metrics_ERA5_GWA_df.to_excel(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_GWA_results.xlsx\", index=False)\n",
    "metrics_ERA5_df.to_excel(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_results.xlsx\", index=False)\n",
    "metrics_ICON_df.to_excel(r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ICON_LAM_results.xlsx\", index=False)\n",
    "\n",
    "print(\"Metrics ICON_LAM\")\n",
    "print(metrics_ICON_df.head())\n",
    "print(\"Metrics ERA5\")\n",
    "print(metrics_ERA5_df.head())\n",
    "print(\"Metrics ERA5_GWA\")\n",
    "print(metrics_ERA5_GWA_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6494c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_extremes(df, dataset_name):\n",
    "    print(f\"\\n📊 Dataset: {dataset_name}\")\n",
    "    metrics = [\"MAE\", \"ME\", \"Pearson r\", \"PSS\"]\n",
    "\n",
    "    for metric in metrics:\n",
    "        print(f\"\\n🔹 Metric: {metric}\")\n",
    "\n",
    "        # Max value\n",
    "        max_row = df.loc[df[metric].idxmax()]\n",
    "        print(f\"⬆️ Highest {metric}:\")\n",
    "        print(max_row.to_string())\n",
    "\n",
    "        # Min value\n",
    "        min_row = df.loc[df[metric].idxmin()]\n",
    "        print(f\"\\n⬇️ Lowest {metric}:\")\n",
    "        print(min_row.to_string())\n",
    "\n",
    "# 📥 Apply to each dataset\n",
    "print_extremes(metrics_ICON_df, \"ICON-LAM\")\n",
    "print_extremes(metrics_ERA5_df, \"ERA5\")\n",
    "print_extremes(metrics_ERA5_GWA_df, \"ERA5_GWA\")\n",
    "\n",
    "\n",
    "\n",
    "def count_mae_intervals(df, dataset_name):\n",
    "    print(f\"\\n📊 MAE Intervals for {dataset_name}\")\n",
    "\n",
    "    # Définir les intervalles\n",
    "    bins = [0, 1, 2, 4, float('inf')]\n",
    "    labels = [\"[0–1]\", \"(1–2]\", \"(2–4]\", \">4\"]\n",
    "\n",
    "    # Créer une nouvelle colonne avec les intervalles\n",
    "    df[\"MAE_interval\"] = pd.cut(df[\"MAE\"], bins=bins, labels=labels, right=True)\n",
    "\n",
    "    # Compter les occurrences par intervalle\n",
    "    counts = df[\"MAE_interval\"].value_counts().sort_index()\n",
    "\n",
    "    # Afficher les résultats\n",
    "    for label in labels:\n",
    "        print(f\"{label}: {counts.get(label, 0)} stations\")\n",
    "\n",
    "# 📥 Appliquer à chaque dataset\n",
    "count_mae_intervals(metrics_ICON_df, \"ICON-LAM\")\n",
    "count_mae_intervals(metrics_ERA5_df, \"ERA5\")\n",
    "count_mae_intervals(metrics_ERA5_GWA_df, \"ERA5_GWA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e48ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Liste des DataFrames à traiter\n",
    "metrics_dataset_dfs = {\n",
    "    \"ICON-LAM\": metrics_ICON_df,\n",
    "    \"ERA5\": metrics_ERA5_df,\n",
    "    \"ERA5_GWA\": metrics_ERA5_GWA_df\n",
    "}\n",
    "\n",
    "# 🔹 Colonnes à moyenner\n",
    "metric_columns = [\"MAE\", \"ME\", \"Pearson r\", \"PSS\"]\n",
    "\n",
    "# 🔹 Initialiser un tableau de résultats\n",
    "summary_means_dataset = pd.DataFrame(columns=metric_columns)\n",
    "\n",
    "# 🔹 Calculer la moyenne pour chaque jeu de données\n",
    "for name, df in metrics_dataset_dfs.items():\n",
    "    summary_means_dataset.loc[name] = df[metric_columns].mean()\n",
    "\n",
    "# 🔹 Afficher le résumé des moyennes\n",
    "print(\"📊 Metric Average from each dataset  :\")\n",
    "print(summary_means_dataset)\n",
    "\n",
    "# 🔹 Définir les quantiles que tu veux\n",
    "quantiles = [0.10, 0.20, 0.25, 0.30, 0.40, 0.50, 0.60, 0.70, 0.75, 0.80, 0.90, 0.95]\n",
    "\n",
    "# 🔹 Colonnes des métriques\n",
    "metrics = [\"MAE\", \"ME\", \"Pearson r\", \"PSS\"]\n",
    "\n",
    "# 🔹 Calculer les quantiles globalement (sur toutes les stations)\n",
    "quantile_ICON_df = metrics_ICON_df[metrics].quantile(quantiles)\n",
    "\n",
    "# 🔹 Nettoyer l'affichage\n",
    "quantile_ICON_df.index = [f\"{int(q*100)}%\" for q in quantiles]\n",
    "quantile_ICON_df.index.name = \"Quantile\"\n",
    "\n",
    "# 🔹 Afficher le tableau\n",
    "print(\"📊 Quantiles globaux pour les métriques ICON-LAM :\")\n",
    "print(quantile_ICON_df)\n",
    "\n",
    "\n",
    "# 🔹 Colonnes des métriques\n",
    "metrics = [\"MAE\", \"ME\", \"Pearson r\", \"PSS\"]\n",
    "\n",
    "# 🔹 Calculer les quantiles globalement (sur toutes les stations)\n",
    "quantile_ERA5_df = metrics_ERA5_df[metrics].quantile(quantiles)\n",
    "\n",
    "# 🔹 Nettoyer l'affichage\n",
    "quantile_ERA5_df.index = [f\"{int(q*100)}%\" for q in quantiles]\n",
    "quantile_ERA5_df.index.name = \"Quantile\"\n",
    "\n",
    "# 🔹 Afficher le tableau\n",
    "print(\"📊 Quantiles globaux pour les métriques ERA5 :\")\n",
    "print(quantile_ERA5_df)\n",
    "\n",
    "\n",
    "# 🔹 Colonnes des métriques\n",
    "metrics = [\"MAE\", \"ME\", \"Pearson r\", \"PSS\"]\n",
    "\n",
    "# 🔹 Calculer les quantiles globalement (sur toutes les stations)\n",
    "quantile_ERA5_GWA_df = metrics_ERA5_GWA_df[metrics].quantile(quantiles)\n",
    "\n",
    "# 🔹 Nettoyer l'affichage\n",
    "quantile_ERA5_GWA_df.index = [f\"{int(q*100)}%\" for q in quantiles]\n",
    "quantile_ERA5_GWA_df.index.name = \"Quantile\"\n",
    "\n",
    "# 🔹 Afficher le tableau\n",
    "print(\"📊 Quantiles globaux pour les métriques ERA5_GWA :\")\n",
    "print(quantile_ERA5_GWA_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# === Chargement des données ===\n",
    "metrics_files = {\n",
    "    \"ICON-LAM\": r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ICON_LAM_results.csv\",\n",
    "    \"ERA5\": r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_results.csv\",\n",
    "    \"ERA5_GWA\": r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\Metrics\\metrics_10m_ERA5_GWA_results.csv\"\n",
    "}\n",
    "metrics_data = {name: pd.read_csv(path) for name, path in metrics_files.items()}\n",
    "station_shapes = {\"SASSCALWN\": \"s\", \"TAHMO\": \"^\", \"NCEI\": \"o\"}\n",
    "\n",
    "world = gpd.read_file(\"https://naturalearth.s3.amazonaws.com/10m_cultural/ne_10m_admin_0_countries.zip\")\n",
    "africa_south = world[world[\"NAME\"].isin([\n",
    "    \"South Africa\", \"Namibia\", \"Botswana\", \"Zimbabwe\", \"Mozambique\",\n",
    "    \"Angola\", \"Lesotho\", \"Eswatini\", \"Zambia\"\n",
    "])]\n",
    "\n",
    "# === Métrique ciblée\n",
    "metric = \"MAE\"\n",
    "title = \"Mean Absolute Error (MAE)\"\n",
    "# Plages de valeurs\n",
    "bounds = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "n_colors = len(bounds) - 1\n",
    "\n",
    "# Couleurs correspondantes (autant que d'intervalles)\n",
    "custom_colors = ['#253494', '#41b6c4', '#ffffcc',\n",
    "                 '#fed976', '#f03b20', '#bd0026'][:n_colors]\n",
    "\n",
    "# Colormap personnalisée\n",
    "cmap = mcolors.ListedColormap(custom_colors)\n",
    "\n",
    "# === Récupérer toutes les valeurs (pour une échelle cohérente)\n",
    "all_values = []\n",
    "\n",
    "\n",
    "for df in metrics_data.values():\n",
    "    all_values.extend(df[metric].dropna().values)\n",
    "norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n",
    "\n",
    "# === Création des sous-cartes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 20), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=0.12, bottom=0.4, top=0.98)\n",
    "\n",
    "for ax, (name, df) in zip(axes, metrics_data.items()):\n",
    "    africa_south.plot(ax=ax, color=\"whitesmoke\", edgecolor=\"black\")\n",
    "    for label, shape in station_shapes.items():\n",
    "        subset = df[df[\"label\"] == label]\n",
    "        ax.scatter(\n",
    "            subset[\"longitude\"], subset[\"latitude\"],\n",
    "            c=subset[metric], cmap=cmap, norm=norm,\n",
    "            s=100, marker=shape, edgecolor=\"black\"\n",
    "        )\n",
    "        # 🔁 Graduation personnalisée pour latitude\n",
    "    lat_ticks = [-42, -35, -30, -25, -20, -15, -10]  # Valeurs en °S\n",
    "    lat_labels = [f\"{abs(val)}°S\" for val in lat_ticks]\n",
    "        # 🔁 Graduation personnalisée pour longitude\n",
    "    lon_ticks = [10, 15, 20, 25, 30, 35, 40]  # Valeurs en °E\n",
    "    lon_labels = [f\"{val}°E\" for val in lon_ticks]\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"{name}\", fontsize=20)\n",
    "    ax.set_xticks(lon_ticks)\n",
    "    ax.set_yticks(lat_ticks)\n",
    "    ax.set_xticklabels(lon_labels, fontsize=14)\n",
    "    ax.set_yticklabels(lat_labels, fontsize=14)\n",
    "    # Limiter l'affichage à la zone géographique ciblée\n",
    "    ax.set_xlim(min(lon_ticks), max(lon_ticks))\n",
    "    ax.set_ylim(min(lat_ticks), max(lat_ticks))\n",
    "    ax.set_xlabel(\"Longitude\", fontsize=16)\n",
    "    ax.set_ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "# ✅ Barre de couleurs\n",
    "cbar_ax = fig.add_axes([0.2, 0.48, 0.5, 0.02])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                    cax=cbar_ax, orientation='horizontal', extend='max')\n",
    "cbar.set_label(\"MAE in [m/s]\", loc='right', fontsize=16)\n",
    "cbar.ax.xaxis.set_label_position('top') \n",
    "cbar.ax.tick_params(labelsize=14)  # ou 16, 18 selon ton besoin\n",
    "\n",
    "    # 🔹 Légende des réseaux\n",
    "legend_elements = [plt.Line2D([0], [0], marker=station_shapes[lab], color='black',\n",
    "                                  linestyle='None', label=lab, markersize=14,\n",
    "                                  markerfacecolor='white', markeredgecolor='black')\n",
    "                       for lab in station_shapes]\n",
    "ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=14)\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "save_path = r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\Compared_Dataset_MAE_10m.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Métrique ciblée\n",
    "metric = \"ME\"\n",
    "title = \"Mean  Error (ME)\"\n",
    "# Plages de valeurs\n",
    "bounds = [-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0,]\n",
    "n_colors = len(bounds) - 1\n",
    "\n",
    "# Couleurs correspondantes (autant que d'intervalles)\n",
    "custom_colors = ['#253494', '#2c7fb8', '#41b6c4', '#a1dab4', '#ffffcc',\n",
    "                 '#fed976', '#feb24c', '#f03b20', '#bd0026'][:n_colors]\n",
    "# Colormap personnalisée\n",
    "cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "# === Récupérer toutes les valeurs (pour une échelle cohérente)\n",
    "all_values = []\n",
    "\n",
    "\n",
    "for df in metrics_data.values():\n",
    "    all_values.extend(df[metric].dropna().values)\n",
    "norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n",
    "\n",
    "# === Création des sous-cartes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 20), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=0.12, bottom=0.4, top=0.98)\n",
    "\n",
    "for ax, (name, df) in zip(axes, metrics_data.items()):\n",
    "    africa_south.plot(ax=ax, color=\"whitesmoke\", edgecolor=\"black\")\n",
    "    for label, shape in station_shapes.items():\n",
    "        subset = df[df[\"label\"] == label]\n",
    "        ax.scatter(\n",
    "            subset[\"longitude\"], subset[\"latitude\"],\n",
    "            c=subset[metric], cmap=cmap, norm=norm,\n",
    "            s=100, marker=shape, edgecolor=\"black\"\n",
    "        )\n",
    "        # 🔁 Graduation personnalisée pour latitude\n",
    "    lat_ticks = [-42, -35, -30, -25, -20, -15, -10]  # Valeurs en °S\n",
    "    lat_labels = [f\"{abs(val)}°S\" for val in lat_ticks]\n",
    "        # 🔁 Graduation personnalisée pour longitude\n",
    "    lon_ticks = [10, 15, 20, 25, 30, 35, 40]  # Valeurs en °E\n",
    "    lon_labels = [f\"{val}°E\" for val in lon_ticks]\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"{name}\", fontsize=20)\n",
    "    ax.set_xticks(lon_ticks)\n",
    "    ax.set_yticks(lat_ticks)\n",
    "    ax.set_xticklabels(lon_labels, fontsize=14)\n",
    "    ax.set_yticklabels(lat_labels, fontsize=14)\n",
    "    # Limiter l'affichage à la zone géographique ciblée\n",
    "    ax.set_xlim(min(lon_ticks), max(lon_ticks))\n",
    "    ax.set_ylim(min(lat_ticks), max(lat_ticks))\n",
    "    ax.set_xlabel(\"Longitude\", fontsize=16)\n",
    "    ax.set_ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "# ✅ Barre de couleurs\n",
    "cbar_ax = fig.add_axes([0.2, 0.48, 0.5, 0.02])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                    cax=cbar_ax, orientation='horizontal', extend = 'both')\n",
    "cbar.set_label(\"ME in [m/s]\", loc='right', fontsize=16)\n",
    "cbar.ax.xaxis.set_label_position('top') \n",
    "cbar.ax.tick_params(labelsize=14)  # ou 16, 18 selon ton besoin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 🔹 Légende des réseaux\n",
    "legend_elements = [plt.Line2D([0], [0], marker=station_shapes[lab], color='black',\n",
    "                                  linestyle='None', label=lab, markersize=14,\n",
    "                                  markerfacecolor='white', markeredgecolor='black')\n",
    "                       for lab in station_shapes]\n",
    "ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=14)\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "save_path = r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\Compared_Dataset_ME_10m.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Métrique ciblée\n",
    "metric = \"Pearson r\"\n",
    "title = \"Pearson Correlation (r)\"\n",
    "# Plages de valeurs\n",
    "bounds = [0.0, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "n_colors = len(bounds) - 1\n",
    "\n",
    "# Couleurs correspondantes (autant que d'intervalles)\n",
    "custom_colors = ['#bd0026', '#f03b20', '#feb24c', '#fed976', '#ffffcc', '#a1dab4', '#41b6c4', '#2c7fb8', '#253494'][:n_colors]\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn_r')\n",
    "\n",
    "# === Récupérer toutes les valeurs (pour une échelle cohérente)\n",
    "all_values = []\n",
    "\n",
    "\n",
    "for df in metrics_data.values():\n",
    "    all_values.extend(df[metric].dropna().values)\n",
    "norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n",
    "\n",
    "# === Création des sous-cartes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 20), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=0.12, bottom=0.4, top=0.98)\n",
    "\n",
    "for ax, (name, df) in zip(axes, metrics_data.items()):\n",
    "    africa_south.plot(ax=ax, color=\"whitesmoke\", edgecolor=\"black\")\n",
    "    for label, shape in station_shapes.items():\n",
    "        subset = df[df[\"label\"] == label]\n",
    "        ax.scatter(\n",
    "            subset[\"longitude\"], subset[\"latitude\"],\n",
    "            c=subset[metric], cmap=cmap, norm=norm,\n",
    "            s=100, marker=shape, edgecolor=\"black\"\n",
    "        )\n",
    "        # 🔁 Graduation personnalisée pour latitude\n",
    "    lat_ticks = [-42, -35, -30, -25, -20, -15, -10]  # Valeurs en °S\n",
    "    lat_labels = [f\"{abs(val)}°S\" for val in lat_ticks]\n",
    "        # 🔁 Graduation personnalisée pour longitude\n",
    "    lon_ticks = [10, 15, 20, 25, 30, 35, 40]  # Valeurs en °E\n",
    "    lon_labels = [f\"{val}°E\" for val in lon_ticks]\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"{name}\", fontsize=20)\n",
    "    ax.set_xticks(lon_ticks)\n",
    "    ax.set_yticks(lat_ticks)\n",
    "    ax.set_xticklabels(lon_labels, fontsize=14)\n",
    "    ax.set_yticklabels(lat_labels, fontsize=14)\n",
    "    # Limiter l'affichage à la zone géographique ciblée\n",
    "    ax.set_xlim(min(lon_ticks), max(lon_ticks))\n",
    "    ax.set_ylim(min(lat_ticks), max(lat_ticks))\n",
    "    ax.set_xlabel(\"Longitude\", fontsize=16)\n",
    "    ax.set_ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "# ✅ Barre de couleurs\n",
    "cbar_ax = fig.add_axes([0.2, 0.48, 0.5, 0.02])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                    cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label(\"Pearson_r [-]\", loc='right', fontsize=16)\n",
    "cbar.ax.xaxis.set_label_position('top') \n",
    "cbar.ax.tick_params(labelsize=14)  # ou 16, 18 selon ton besoin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 🔹 Légende des réseaux\n",
    "legend_elements = [plt.Line2D([0], [0], marker=station_shapes[lab], color='black',\n",
    "                                  linestyle='None', label=lab, markersize=14,\n",
    "                                  markerfacecolor='white', markeredgecolor='black')\n",
    "                       for lab in station_shapes]\n",
    "ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=14)\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "save_path = r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\Compared_Dataset_Pearson_r_10m.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Métrique ciblée\n",
    "metric = \"PSS\"\n",
    "title = \"Perkins Skill Score (PSS)\"\n",
    "# Plages de valeurs\n",
    "bounds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "n_colors = len(bounds) - 1\n",
    "\n",
    "custom_colors = [\n",
    "    '#f0f0f0',  # 0.0 : gris très clair\n",
    "    '#d9d9d9',  # 0.1 : gris clair\n",
    "    '#ffffcc',  # 0.2 : jaune pâle\n",
    "    '#ffeda0',  # 0.3 : jaune doux\n",
    "    '#fed976',  # 0.4 : jaune plus chaud\n",
    "    '#c5dbf0',  # 0.5 : transition vers bleu clair\n",
    "    '#9ecae1',  # 0.6 : bleu doux\n",
    "    '#6baed6',  # 0.7 : bleu modéré\n",
    "    '#3182bd',  # 0.8 : bleu foncé\n",
    "    '#08519c'   # 0.9–1.0 : bleu profond\n",
    "]\n",
    "\n",
    "# Colormap personnalisée\n",
    "cmap = mcolors.ListedColormap(custom_colors)\n",
    "\n",
    "# === Récupérer toutes les valeurs (pour une échelle cohérente)\n",
    "all_values = []\n",
    "\n",
    "\n",
    "for df in metrics_data.values():\n",
    "    all_values.extend(df[metric].dropna().values)\n",
    "norm = mcolors.BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n",
    "\n",
    "# === Création des sous-cartes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 20), sharex=True, sharey=True)\n",
    "plt.subplots_adjust(wspace=0.12, bottom=0.4, top=0.98)\n",
    "\n",
    "for ax, (name, df) in zip(axes, metrics_data.items()):\n",
    "    africa_south.plot(ax=ax, color=\"whitesmoke\", edgecolor=\"black\")\n",
    "    for label, shape in station_shapes.items():\n",
    "        subset = df[df[\"label\"] == label]\n",
    "        ax.scatter(\n",
    "            subset[\"longitude\"], subset[\"latitude\"],\n",
    "            c=subset[metric], cmap=cmap, norm=norm,\n",
    "            s=100, marker=shape, edgecolor=\"black\"\n",
    "        )\n",
    "        # 🔁 Graduation personnalisée pour latitude\n",
    "    lat_ticks = [-42, -35, -30, -25, -20, -15, -10]  # Valeurs en °S\n",
    "    lat_labels = [f\"{abs(val)}°S\" for val in lat_ticks]\n",
    "        # 🔁 Graduation personnalisée pour longitude\n",
    "    lon_ticks = [10, 15, 20, 25, 30, 35, 40]  # Valeurs en °E\n",
    "    lon_labels = [f\"{val}°E\" for val in lon_ticks]\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"{name}\", fontsize=20)\n",
    "    ax.set_xticks(lon_ticks)\n",
    "    ax.set_yticks(lat_ticks)\n",
    "    ax.set_xticklabels(lon_labels, fontsize=14)\n",
    "    ax.set_yticklabels(lat_labels, fontsize=14)\n",
    "    # Limiter l'affichage à la zone géographique ciblée\n",
    "    ax.set_xlim(min(lon_ticks), max(lon_ticks))\n",
    "    ax.set_ylim(min(lat_ticks), max(lat_ticks))\n",
    "    ax.set_xlabel(\"Longitude\", fontsize=16)\n",
    "    ax.set_ylabel(\"Latitude\", fontsize=16)\n",
    "\n",
    "# ✅ Barre de couleurs\n",
    "cbar_ax = fig.add_axes([0.2, 0.48, 0.5, 0.02])\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n",
    "                    cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label(\"PSS [-]\", loc='right', fontsize=16)\n",
    "cbar.ax.xaxis.set_label_position('top') \n",
    "cbar.ax.tick_params(labelsize=14)  # ou 16, 18 selon ton besoin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 🔹 Légende des réseaux\n",
    "legend_elements = [plt.Line2D([0], [0], marker=station_shapes[lab], color='black',\n",
    "                                  linestyle='None', label=lab, markersize=14,\n",
    "                                  markerfacecolor='white', markeredgecolor='black')\n",
    "                       for lab in station_shapes]\n",
    "ax.legend(handles=legend_elements, loc='lower right', frameon=True, fontsize=14)\n",
    "\n",
    "\n",
    "# Sauvegarde\n",
    "save_path = r\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\Compared_Dataset_PSS_10m.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 Dictionnaire des datasets\n",
    "datasets = {\n",
    "    \"ICON-LAM\": metrics_ICON_df.copy(),\n",
    "    \"ERA5\": metrics_ERA5_df.copy(),\n",
    "    \"ERA5_GWA\": metrics_ERA5_GWA_df.copy()\n",
    "}\n",
    "\n",
    "# 🔧 Palette et ordre\n",
    "label_order = [\"SASSCALWN\", \"TAHMO\", \"NCEI\", \"ALL\"]\n",
    "palette = {\n",
    "    \"SASSCALWN\": \"#66c2a5\",\n",
    "    \"TAHMO\": \"gray\",\n",
    "    \"NCEI\": \"#8da0cb\",\n",
    "    \"ALL\": \"#e78ac3\"\n",
    "}\n",
    "\n",
    "# 🔧 Métriques à tracer\n",
    "metrics = {\n",
    "    \"MAE\": \"Mean Absolute Error (m/s)\"\n",
    "}\n",
    "\n",
    "# 🔁 Boucle sur chaque métrique\n",
    "for metric, metric_label in metrics.items():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7), sharey=True)\n",
    "    plt.suptitle(f\"{metric_label} – Dataset vs Observed at 10m\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for ax, (ds_name, df) in zip(axes, datasets.items()):\n",
    "        # 🔧 Ajouter catégorie ALL\n",
    "        df_all = df.copy()\n",
    "        df_all[\"label\"] = \"ALL\"\n",
    "        df_combined = pd.concat([df, df_all], ignore_index=True)\n",
    "\n",
    "        # 📊 Boxplot\n",
    "        sns.boxplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            palette=palette,\n",
    "            showfliers=False,\n",
    "            width=0.6,\n",
    "            boxprops=dict(linewidth=1.2, edgecolor='black'),\n",
    "            medianprops=dict(color=\"black\", linewidth=2),\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Points individuels\n",
    "        sns.stripplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            color=\"orange\", size=5, alpha=0.6,\n",
    "            jitter=True,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Moyenne en ligne blanche pointillée + n\n",
    "        for i, label in enumerate(label_order):\n",
    "            vals = df_combined[df_combined[\"label\"] == label][metric].dropna()\n",
    "            if not vals.empty:\n",
    "                mean_val = vals.mean()\n",
    "                ax.hlines(mean_val, i - 0.2, i + 0.2, colors=\"white\", linestyles=\"--\", linewidth=2)\n",
    "                ax.text(i, vals.max() + 0.05 * abs(vals.max()), f\"n={len(vals)}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        ax.set_title(ds_name, fontsize=18)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"MAE(m/s)\", fontsize=16)\n",
    "        ax.set_xticklabels(label_order, rotation=20, fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = f\"BoxPlot_GROUPED_10m_{metric.replace(' ', '_')}.png\"\n",
    "    save_path = rf\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\{filename}\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 🔧 Métriques à tracer\n",
    "metrics = {\n",
    "    \"ME\": \"Mean Error (m/s)\"\n",
    "}\n",
    "\n",
    "# 🔁 Boucle sur chaque métrique\n",
    "for metric, metric_label in metrics.items():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7), sharey=True)\n",
    "    plt.suptitle(f\"{metric_label} – Dataset vs Observed at 10m\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for ax, (ds_name, df) in zip(axes, datasets.items()):\n",
    "        # 🔧 Ajouter catégorie ALL\n",
    "        df_all = df.copy()\n",
    "        df_all[\"label\"] = \"ALL\"\n",
    "        df_combined = pd.concat([df, df_all], ignore_index=True)\n",
    "\n",
    "        # 📊 Boxplot\n",
    "        sns.boxplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            palette=palette,\n",
    "            showfliers=False,\n",
    "            width=0.6,\n",
    "            boxprops=dict(linewidth=1.2, edgecolor='black'),\n",
    "            medianprops=dict(color=\"black\", linewidth=2),\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Points individuels\n",
    "        sns.stripplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            color=\"orange\", size=5, alpha=0.6,\n",
    "            jitter=True,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Moyenne en ligne blanche pointillée + n\n",
    "        for i, label in enumerate(label_order):\n",
    "            vals = df_combined[df_combined[\"label\"] == label][metric].dropna()\n",
    "            if not vals.empty:\n",
    "                mean_val = vals.mean()\n",
    "                ax.hlines(mean_val, i - 0.2, i + 0.2, colors=\"white\", linestyles=\"--\", linewidth=2)\n",
    "                ax.text(i, vals.max() + 0.05 * abs(vals.max()), f\"n={len(vals)}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        ax.set_title(ds_name, fontsize=18)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"ME(m/s)\", fontsize=16)\n",
    "        ax.set_xticklabels(label_order, rotation=20, fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = f\"BoxPlot_GROUPED_10m_{metric.replace(' ', '_')}.png\"\n",
    "    save_path = rf\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\{filename}\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 🔧 Métriques à tracer\n",
    "metrics = {\n",
    "    \"Pearson r\": \"Pearson Correlation\"\n",
    "}\n",
    "\n",
    "# 🔁 Boucle sur chaque métrique\n",
    "for metric, metric_label in metrics.items():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7), sharey=True)\n",
    "    plt.suptitle(f\"{metric_label} – Dataset vs Observed at 10m\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for ax, (ds_name, df) in zip(axes, datasets.items()):\n",
    "        # 🔧 Ajouter catégorie ALL\n",
    "        df_all = df.copy()\n",
    "        df_all[\"label\"] = \"ALL\"\n",
    "        df_combined = pd.concat([df, df_all], ignore_index=True)\n",
    "\n",
    "        # 📊 Boxplot\n",
    "        sns.boxplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            palette=palette,\n",
    "            showfliers=False,\n",
    "            width=0.6,\n",
    "            boxprops=dict(linewidth=1.2, edgecolor='black'),\n",
    "            medianprops=dict(color=\"black\", linewidth=2),\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Points individuels\n",
    "        sns.stripplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            color=\"orange\", size=5, alpha=0.6,\n",
    "            jitter=True,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Moyenne en ligne blanche pointillée + n\n",
    "        for i, label in enumerate(label_order):\n",
    "            vals = df_combined[df_combined[\"label\"] == label][metric].dropna()\n",
    "            if not vals.empty:\n",
    "                mean_val = vals.mean()\n",
    "                ax.hlines(mean_val, i - 0.2, i + 0.2, colors=\"white\", linestyles=\"--\", linewidth=2)\n",
    "                ax.text(i, vals.max() + 0.05 * abs(vals.max()), f\"n={len(vals)}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        ax.set_title(ds_name, fontsize=18)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Pearson r[:]\", fontsize=16)\n",
    "        ax.set_xticklabels(label_order, rotation=20, fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = f\"BoxPlot_GROUPED_10m_{metric.replace(' ', '_')}.png\"\n",
    "    save_path = rf\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\{filename}\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 🔧 Métriques à tracer\n",
    "metrics = {\n",
    "    \"PSS\": \"Perkins Skill Score\"\n",
    "}\n",
    "\n",
    "# 🔁 Boucle sur chaque métrique\n",
    "for metric, metric_label in metrics.items():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 7), sharey=True)\n",
    "    plt.suptitle(f\"{metric_label} – Dataset vs Observed at 10m\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    for ax, (ds_name, df) in zip(axes, datasets.items()):\n",
    "        # 🔧 Ajouter catégorie ALL\n",
    "        df_all = df.copy()\n",
    "        df_all[\"label\"] = \"ALL\"\n",
    "        df_combined = pd.concat([df, df_all], ignore_index=True)\n",
    "\n",
    "        # 📊 Boxplot\n",
    "        sns.boxplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            palette=palette,\n",
    "            showfliers=False,\n",
    "            width=0.6,\n",
    "            boxprops=dict(linewidth=1.2, edgecolor='black'),\n",
    "            medianprops=dict(color=\"black\", linewidth=2),\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Points individuels\n",
    "        sns.stripplot(\n",
    "            data=df_combined,\n",
    "            x=\"label\", y=metric,\n",
    "            order=label_order,\n",
    "            color=\"orange\", size=5, alpha=0.6,\n",
    "            jitter=True,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        # 📍 Moyenne en ligne blanche pointillée + n\n",
    "        for i, label in enumerate(label_order):\n",
    "            vals = df_combined[df_combined[\"label\"] == label][metric].dropna()\n",
    "            if not vals.empty:\n",
    "                mean_val = vals.mean()\n",
    "                ax.hlines(mean_val, i - 0.2, i + 0.2, colors=\"white\", linestyles=\"--\", linewidth=2)\n",
    "                ax.text(i, vals.max() + 0.05 * abs(vals.max()), f\"n={len(vals)}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        ax.set_title(ds_name, fontsize=18)\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"PSS[:]\", fontsize=16)\n",
    "        ax.set_xticklabels(label_order, rotation=20, fontsize=16)\n",
    "        ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    filename = f\"BoxPlot_GROUPED_10m_{metric.replace(' ', '_')}.png\"\n",
    "    save_path = rf\"C:\\Users\\LENOVO\\Downloads\\pyton trainning\\essais perso\\week 16\\Final coding\\plot\\10m\\{filename}\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd42afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdedab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d4bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (windsim)",
   "language": "python",
   "name": "windsim_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
